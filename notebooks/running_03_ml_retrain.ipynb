{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === running_03_ml_combined.ipynb â€” v3 models from merged data ===\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error, r2_score,\n",
    "    classification_report, confusion_matrix, roc_auc_score\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential, layers, callbacks\n",
    "\n",
    "# -------------------------\n",
    "# 0) Paths\n",
    "# -------------------------\n",
    "ROOT     = Path(\"/Users/amlim/triathlon-performance\")\n",
    "SRC_EXCL = ROOT / \"data\" / \"cleaned_running.csv\"              # Excel-based (earlier)\n",
    "SRC_GPX  = ROOT / \"data\" / \"cleaned_running_from_gpx.csv\"     # GPX-derived (new)\n",
    "MERGED   = ROOT / \"data\" / \"cleaned_running_merged.csv\"\n",
    "OUT_DIR  = ROOT / \"results\" / \"models\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "LOG_CSV  = ROOT / \"results\" / \"model_performance_log.csv\"\n",
    "\n",
    "# -------------------------\n",
    "# 1) Load & merge (union of columns)\n",
    "# -------------------------\n",
    "def safe_read(p):\n",
    "    try:\n",
    "        return pd.read_csv(p)\n",
    "    except Exception:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "df1 = safe_read(SRC_EXCL)\n",
    "df2 = safe_read(SRC_GPX)\n",
    "\n",
    "if df1.empty and df2.empty:\n",
    "    raise ValueError(\"Both source files are empty/missing. Provide at least one.\")\n",
    "\n",
    "# Normalize common column names where they may differ\n",
    "rename_map = {\n",
    "    \"start_time\": \"time\",           # GPX summary uses start_time\n",
    "    \"elev_gain\": \"elev_gain_m\",     # just in case\n",
    "}\n",
    "df1 = df1.rename(columns=rename_map)\n",
    "df2 = df2.rename(columns=rename_map)\n",
    "\n",
    "# Ensure presence of expected columns\n",
    "expected = [\n",
    "    \"date\",\"time\",\"time_of_day\",\"indoor_outdoor\",\n",
    "    \"distance_km\",\"duration_min\",\"avg_pace_min_per_km\",\n",
    "    \"elev_gain_m\",\"average_heartrate\",\"max_heartrate\",\"calories\"\n",
    "]\n",
    "for c in expected:\n",
    "    if c not in df1.columns: df1[c] = np.nan\n",
    "    if c not in df2.columns: df2[c] = np.nan\n",
    "\n",
    "# Concatenate and tidy\n",
    "df = pd.concat([df1[expected], df2[expected]], ignore_index=True)\n",
    "\n",
    "# Coerce numerics\n",
    "num_cols = [\"distance_km\",\"duration_min\",\"avg_pace_min_per_km\",\"elev_gain_m\",\n",
    "            \"average_heartrate\",\"max_heartrate\",\"calories\"]\n",
    "for c in num_cols:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "# Basic sanity filters (tweak as needed)\n",
    "df = df[(df[\"distance_km\"] > 0) & (df[\"duration_min\"] > 0)]\n",
    "df = df[(df[\"avg_pace_min_per_km\"] > 3) & (df[\"avg_pace_min_per_km\"] < 12)]  # ~3â€“12 min/km\n",
    "df[\"time_of_day\"]     = df[\"time_of_day\"].astype(str).str.strip().str.title()\n",
    "df[\"indoor_outdoor\"]  = df[\"indoor_outdoor\"].astype(str).str.strip().str.lower().replace(\n",
    "    {\"indoors\":\"indoor\",\"outdoors\":\"outdoor\"}\n",
    ")\n",
    "\n",
    "# Re-label effectiveness on the MERGED set (top 25% fastest)\n",
    "q25 = df[\"avg_pace_min_per_km\"].quantile(0.25)\n",
    "df[\"high_effectiveness_run\"] = (df[\"avg_pace_min_per_km\"] <= q25).astype(int)\n",
    "\n",
    "# Save merged dataset\n",
    "df.to_csv(MERGED, index=False)\n",
    "print(f\"âœ… Merged dataset saved: {MERGED}  rows={len(df)}\")\n",
    "\n",
    "# -------------------------\n",
    "# -------------------------\n",
    "# 2) Features & encoding (robust)\n",
    "# -------------------------\n",
    "# One-hot categoricals if they have >1 category\n",
    "cat_cols = []\n",
    "if df[\"time_of_day\"].nunique() > 1:    cat_cols.append(\"time_of_day\")\n",
    "if df[\"indoor_outdoor\"].nunique() > 1: cat_cols.append(\"indoor_outdoor\")\n",
    "\n",
    "df_ml = pd.get_dummies(df, columns=cat_cols, drop_first=True)\n",
    "\n",
    "# Core features that should exist and be fairly complete\n",
    "core_feats = [c for c in [\"distance_km\",\"duration_min\",\"elev_gain_m\"] if c in df_ml.columns]\n",
    "\n",
    "# Optional features â€” include only if they have enough non-nulls\n",
    "optional_pool = [c for c in [\"average_heartrate\",\"max_heartrate\",\"calories\"] if c in df_ml.columns]\n",
    "nonnull_ratio = df_ml[optional_pool].notna().mean().sort_values(ascending=False) if optional_pool else pd.Series(dtype=float)\n",
    "optional_kept = [c for c in optional_pool if df_ml[c].notna().mean() >= 0.60]\n",
    "\n",
    "# Available dummies (after drop_first some may not exist)\n",
    "dummy_feats = [c for c in df_ml.columns if c.startswith(\"time_of_day_\") or c.startswith(\"indoor_outdoor_\")]\n",
    "\n",
    "X_cols = core_feats + optional_kept + dummy_feats\n",
    "\n",
    "print(\"---- Feature diagnostics ----\")\n",
    "print(\"Candidate optional features and non-null ratio:\")\n",
    "print(nonnull_ratio if not nonnull_ratio.empty else \"  (none)\")\n",
    "print(\"Core kept:\", core_feats)\n",
    "print(\"Optional kept (â‰¥60% non-null):\", optional_kept)\n",
    "print(\"Dummies kept:\", dummy_feats)\n",
    "\n",
    "if not X_cols:\n",
    "    raise ValueError(\"No usable feature columns after checks. Inspect df_ml.columns and data completeness.\")\n",
    "\n",
    "X     = df_ml[X_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "y_reg = pd.to_numeric(df_ml[\"avg_pace_min_per_km\"], errors=\"coerce\")\n",
    "y_clf = df_ml[\"high_effectiveness_run\"].astype(int)\n",
    "\n",
    "# Show null rates before masking\n",
    "print(\"\\nNull rates in selected X features:\")\n",
    "print(X.isna().mean().sort_values())\n",
    "\n",
    "# Drop rows with any NaN in selected X or y\n",
    "mask = X.notna().all(axis=1) & y_reg.notna() & y_clf.notna()\n",
    "print(\"\\nRows before mask:\", len(X), \" | after mask:\", mask.sum())\n",
    "\n",
    "X, y_reg, y_clf = X.loc[mask], y_reg.loc[mask], y_clf.loc[mask]\n",
    "\n",
    "if len(X) == 0:\n",
    "    raise ValueError(\"All rows dropped after NaN filtering. Consider lowering threshold, imputing, or removing sparse features.\")\n",
    "\n",
    "# -------------------------\n",
    "# 3) Train/test split & scale\n",
    "# -------------------------\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_tr, X_te, y_tr_reg, y_te_reg = train_test_split(X, y_reg, test_size=0.2, random_state=42)\n",
    "X_tr_c, X_te_c, y_tr_clf, y_te_clf = train_test_split(X, y_clf, test_size=0.2, random_state=42, stratify=y_clf)\n",
    "\n",
    "scaler_reg = StandardScaler().fit(X_tr)\n",
    "X_tr_s = scaler_reg.transform(X_tr)\n",
    "X_te_s = scaler_reg.transform(X_te)\n",
    "\n",
    "scaler_clf = StandardScaler().fit(X_tr_c)\n",
    "X_tr_c_s = scaler_clf.transform(X_tr_c)\n",
    "X_te_c_s = scaler_clf.transform(X_te_c)\n",
    "\n",
    "# -------------------------\n",
    "# 4A) Regression â€” v3\n",
    "# -------------------------\n",
    "tf.keras.utils.set_random_seed(42)\n",
    "reg_model = Sequential([\n",
    "    layers.Input(shape=(X_tr_s.shape[1],)),\n",
    "    layers.Dense(64, activation=\"relu\"),\n",
    "    layers.Dense(32, activation=\"relu\"),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "reg_model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "early = callbacks.EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
    "reg_model.fit(X_tr_s, y_tr_reg, validation_split=0.2, epochs=200, batch_size=16, verbose=0, callbacks=[early])\n",
    "\n",
    "yhat = reg_model.predict(X_te_s).ravel()\n",
    "reg_mae = mean_absolute_error(y_te_reg, yhat)\n",
    "reg_r2  = r2_score(y_te_reg, yhat)\n",
    "print(\"\\n=== Regression v3 (min/km) ===\")\n",
    "print(\"MAE:\", round(reg_mae, 4))\n",
    "print(\"RÂ² :\", round(reg_r2, 4))\n",
    "\n",
    "# -------------------------\n",
    "# 4B) Classification â€” v3\n",
    "# -------------------------\n",
    "classes = np.unique(y_tr_clf)\n",
    "cw = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_tr_clf)\n",
    "class_weight = {int(c): float(w) for c, w in zip(classes, cw)}\n",
    "\n",
    "clf_model = Sequential([\n",
    "    layers.Input(shape=(X_tr_c_s.shape[1],)),\n",
    "    layers.Dense(64, activation=\"relu\"),\n",
    "    layers.Dense(32, activation=\"relu\"),\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "clf_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "early_c = callbacks.EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
    "clf_model.fit(X_tr_c_s, y_tr_clf, validation_split=0.2, epochs=150, batch_size=16, verbose=0,\n",
    "              callbacks=[early_c], class_weight=class_weight)\n",
    "\n",
    "probs = clf_model.predict(X_te_c_s).ravel()\n",
    "preds = (probs >= 0.5).astype(int)\n",
    "\n",
    "print(\"\\n=== Classification v3 (High Effectiveness) ===\")\n",
    "print(classification_report(y_te_clf, preds, digits=3))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_te_clf, preds))\n",
    "try:\n",
    "    clf_auc = roc_auc_score(y_te_clf, probs)\n",
    "    print(\"ROC AUC:\", round(clf_auc, 4))\n",
    "except ValueError:\n",
    "    clf_auc = np.nan\n",
    "    print(\"ROC AUC: not defined (one class in y_true).\")\n",
    "\n",
    "# -------------------------\n",
    "# 5) Save artifacts\n",
    "# -------------------------\n",
    "reg_path = OUT_DIR / \"run_pace_reg_v3.keras\"\n",
    "clf_path = OUT_DIR / \"run_effectiveness_clf_v3.keras\"\n",
    "reg_model.save(reg_path)\n",
    "clf_model.save(clf_path)\n",
    "import joblib\n",
    "joblib.dump(scaler_reg, OUT_DIR / \"run_reg_scaler_v3.joblib\")\n",
    "joblib.dump(scaler_clf, OUT_DIR / \"run_clf_scaler_v3.joblib\")\n",
    "\n",
    "print(f\"\\nSaved models:\\n  {reg_path}\\n  {clf_path}\")\n",
    "print(f\"Saved scalers in: {OUT_DIR}\")\n",
    "\n",
    "# -------------------------\n",
    "# 6) Log metrics (append)\n",
    "# -------------------------\n",
    "log_row = pd.DataFrame([{\n",
    "    \"model_version\": \"v3\",\n",
    "    \"n_rows\": len(X),\n",
    "    \"features\": \",\".join(X_cols),\n",
    "    \"reg_mae\": reg_mae,\n",
    "    \"reg_r2\": reg_r2,\n",
    "    \"clf_auc\": float(clf_auc) if not np.isnan(clf_auc) else \"\",\n",
    "}])\n",
    "\n",
    "if LOG_CSV.exists():\n",
    "    old = pd.read_csv(LOG_CSV)\n",
    "    pd.concat([old, log_row], ignore_index=True).to_csv(LOG_CSV, index=False)\n",
    "else:\n",
    "    log_row.to_csv(LOG_CSV, index=False)\n",
    "\n",
    "print(f\"ðŸ“ˆ Metrics logged to: {LOG_CSV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_recall_curve\n",
    "pr, rc, th = precision_recall_curve(y_te_clf, probs)\n",
    "f1 = 2*pr*rc/(pr+rc+1e-9)\n",
    "best_t = th[f1[:-1].argmax()]\n",
    "preds_opt = (probs >= best_t).astype(int)\n",
    "print(\"Best F1 threshold:\", best_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"speed_km_per_min\"] = df[\"distance_km\"] / df[\"duration_min\"]          # linear target alt\n",
    "df[\"pace_inv\"] = 1 / df[\"avg_pace_min_per_km\"]                           # monotonic transform\n",
    "df[\"elev_gain_per_km\"] = df[\"elev_gain_m\"] / df[\"distance_km\"].clip(lower=1e-6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cycling)",
   "language": "python",
   "name": "cycling"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
